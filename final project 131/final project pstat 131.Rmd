---
title: "Final Project Jamie Ki"
output:
  html_document: default
  pdf_document: default
date: '2022-04-23'
---
                                                                                                                                                                                                                                                          
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
library(yardstick)
library(parsnip)
library(rsample)
library(dplyr)
library(tibble)
library(ISLR) 
library(ISLR2) 
library(discrim)
library(corrr)
library(ggplot2)
library(tidyverse)
library(klaR)
library(corrplot)
library(caret)
library(pROC)
library(grid)
library(nnet)
library(corrgram)
library(MLmetrics)
library(gbm)
library(ROCR)
library(rpart.plot)
library(vip)
library(janitor)
library(randomForest)
library(xgboost)
library(glmnet)
library(tidymodels)
tidymodels_prefer()
```

## Introudction

What is breast cancer?

Breast cancer is a type of cancer that forms in the cells of the breasts. After skin cancer, breast cancer is the most common cancer diagnosed in women in the United States. This cancer causes a huge number of deaths every year. Therefore, in this machine learning project, we will use classification and data mining methods to classify which type of cancer the patient has. This will make it easier for doctors to provide the correct treatment and treat patients on time for their survival.

![Fig. 1: Breast Cancer](Users/minjaeki/Documents/bcancer){width="363"}


## Loading Data and Packages

In this machine learning project, we will analyze and classify the Breast Cancer Data set from University of California, Irvine, to determine which breast cancer belongs to which category. Basically, there are two types of breast cancer we will be considering in this project : malignant type breast cancer and benign type breast cancer. Malignant indicates the presence of cancer cells whereas benign indicates the absence of cancer cells. In medical analysis, malignant would represent a positive result whereas benign would represent a negative result.

So the main goal for our project is to create a model to correctly classify whether the Breast Cancer is a malignant or benign type with the help of a data set.


There are 10 columns of our predictors. We will define these terms for clarification.
-`ID`
-`clump_thickness` : grouping of cancer cells in multilayer
-`uniformity_size`: metastasis to lymph nodes
-`uniformity_shape`: cancerous cells of varying size
-`marginal_adhesion` : a sign of malignancy but the cancerous cells lose this property so this retention of adhesion is an indication of malignancy
-`single_epithelial_cell_sizer` : also called SECS, if the SECS become larger, then it may be a malignant cell
-`bare_nuclei` : without cytoplasm coating, this is found in benign cells
-`bland_chromatin` : usually found in benign cells
-`normal_nucleoli` : generally very small and in benign cells
-`mitoses` : the process in cell division by which the nucleus divides
-`diagnosis`: two types in our machine learning project, malignant cancer cell and benign cancer cell


Now, we will download our data set using tidy. We will name our data set `bcancer`. We can see that the data set includes cytological characteristics of fluid samples from 699 patients. 


```{r}
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"

bcancer <- read.csv(url)

str(bcancer)
head(bcancer)

```



## Data Cleaning

Now, we will clean our data. We will first rename our columns into `ID`, `clump_thickness`, `uniformity_size`, `uniformity_shape`, `marginal_adhesion`, `single_epithelial_cell_size`, `bare_nuclei`, `bland_chromatin`, `normal_nucleoli`, `mitoses`, and `diagnosis`. The chart below represents our data with the cleared data and changed predictor names.

```{r}
library(dplyr)
library(tidyverse)


bcancer <- read.csv(file = url, header = FALSE,
                 col.names = c("ID","clump_thickness", "uniformity_size", "uniformity_shape", "marginal_adhesion", "single_epithelial_cell_size", "bare_nuclei", "bland_chromatin", "normal_nucleoli","mitoses", "diagnosis"))

head(bcancer)

```

Now, we will look for missing values. We can see from our previous chart that we have 16 missing values in bare_nuclei. We will try to exclude this value from our project to avoid errors and problems.
```{r}
sum(bcancer$bare_nuclei == "?")
```

We will also exclude the ID number with the 16 missing values in `bare_nuclei`. We are excluding the `ID` number since these are just ID numbers that would not be helpful for our model. We will be naming our new data set `bcancer2`.

```{r}
bcancer2 <- bcancer %>%
  select(-ID, -bare_nuclei)
```

Now, our dependent variable `diagnosis` will be denoted as 2 that represents "benign" and 4 that represents "malignant". We will convert these values into a binary variable, 0 and 1 respectively, for our convenience for our model. 

```{r}
bcancer2 <- bcancer2 %>% 
  mutate(diagnosis = as.factor(ifelse(diagnosis == 2, 0, 1)))
summary(bcancer2)

```

## Plots
Now that we have completed our data cleaning procedure, we will look more in depth into our data. We will first start by visualizing different types of plots to explore the relationships among variables. First, we will find the frequency of cancer diagnosis using a pie chart.


```{r}
# Frequency table for cancer diagnosis
diagnosis.table <- table(bcancer2$diagnosis)
diagnosis.table

#pie chart for cancer diagnosis
library(RColorBrewer)
color <- brewer.pal(2, "Set2") 
diagnosis.prop.table <- prop.table(diagnosis.table)*100
diagnosis.prop.df <- as.data.frame(diagnosis.prop.table)
pielabels <- sprintf("%s - %3.1f%s", diagnosis.prop.df[,1], diagnosis.prop.table, "%")

class(diagnosis.prop.table)
pie(diagnosis.prop.table,
  labels= pielabels,  
  clockwise=TRUE,
  col=color,
  border="gainsboro",
  radius=0.9,
  cex=0.8, 
  main="frequency of cancer diagnosis")
legend(1, 0.9,  c("Malignant", "Benign"), cex = 0.8, fill = color)
color
```

Recall that malignant indicates the presence of cancer cells and benign indicates the absence of cancer cells. We can see that there are 357 observations in malignant, the presence of cancer cells, which is 62.7% and 212 observations for benign, the absence of cancer cells, which is 37.3%. In other words, malignant would represent a positive result and benign would represent a negative result. This is an interesting frequency because it is more typical to have more negative results than positive results in a medical analysis. However, in our case, since we have more malignant, which is a positive result, our data is different than other typical medical analysis.

## Data Split

Before looking more into our data, especially our predictors, we will split our data. We will split our data set into training sample and testing sample. We will name them each `bcancer_train` and `bcancer_test`. Here, we will use a 50/50 split. Also, we will fold our training data using cross-validation. 

```{r}
sample_size = floor(0.5 * nrow(bcancer2))

set.seed(1729)
bcancer_new = sample(seq_len(nrow(bcancer2)), size = sample_size)

bcancer_train = bcancer2[bcancer_new, ]

bcancer_test = bcancer2[-bcancer_new, ]

head(bcancer_train)
head(bcancer_test)

bcancer_fold <- vfold_cv(bcancer_train, v = 3)
bcancer_fold

prop.table(table(bcancer_train$diagnosis))
prop.table(table(bcancer_test$diagnosis))


```
## Recipe

Now we will create a recipe for our data set. We are including steps to fit our models at the end of our project. We will name the recipe of our project `bcancer_recipe`'.

```{r}
bcancer_recipe <- recipe(diagnosis ~ clump_thickness + uniformity_size + uniformity_shape + marginal_adhesion + single_epithelial_cell_size + bland_chromatin + normal_nucleoli + mitoses, data = bcancer_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```


## Exploratory Data Analysis

Now that we split our data and created a recipe, we will explore our entire data set, training data set, and testing data set. 

We explored our entire data set using a pie chart, but for an easier comparison with our training and testing data set, we will look into the distribution of diagnosis of our entire data set using a bar chart this time.

We will first explore our entire data set with a bar chart.

```{r}
ggplot(bcancer2, aes(x=diagnosis, fill=diagnosis )) + 
  geom_bar( ) +
  ggtitle("Distribution of diagnosis for the dataset") + 
  scale_x_discrete(labels=c("Malignant", "Benign")) +
  theme(legend.position="none")

```

This is a similar result to the pie chart we used above. We have more malignant than benign.

Now, we will use a bar chart to explore our training data set. We will look into the distribution of diagnosis.


```{r}

ggplot(bcancer_train, aes(x=diagnosis, fill=diagnosis )) + 
  geom_bar( ) +
  ggtitle("Distribution of diagnosis for the training dataset") + 
  scale_x_discrete(labels=c("Malignant", "Benign")) +
  theme(legend.position="none")



```




We can see that there is more malignant than benign as well, and malignant is approximately 230 observations and benign is approximately 125 observations.

Lastly, we will look into the distribution of diagnosis of our testing data set.

```{r}

ggplot(bcancer_test, aes(x=diagnosis, fill=diagnosis )) + 
  geom_bar( ) +
  ggtitle("Distribution of diagnosis for the testing dataset") + 
  scale_x_discrete(labels=c("Malignant", "Benign")) +
  theme(legend.position="none")

```



We can see that the distribution of our testing data set is similar to our training data set with approximately 230 malignant and 125 benign observations.

Now, we will look more into our predictors. In order to see the relationship between predictors, we will use a Correlogram. By using a Correlogram, we will be able to visualize the interaction between each predictor.

```{r}
# Corrgram of the entire data set
corrgram(bcancer2, order=NULL, lower.panel=panel.shade, upper.panel=NULL, text.panel=panel.txt, main="Correlogram of the data")
```
Note that a darker color represents a strong relationship between predictors. Since we mostly see blue colors, most of the predictors do have an interaction with each other. We can see that `uniformity_size` and `uniformity_shape` have the strongest relationship with each predictor. On the other hand, `mitoses` have the least interaction with other predictors. Other predictors such as `clump_thickness`, `marginal adhesion`, `size_epithelial_cell`, `bland_chromatin`, and `normal_nucleoli` have a decently strong relationship with other predictors. Since `uniformity_size` and `uniformity_shape` have the strongest relationship with other predictors, we can conclude that they would be the significant predictors in our project. 



## Fit model

#### Random Forest

Now, we will fit our model into different model classes. By doing this, we will be able to study our data set more, by learning more about the relationship between the predictors and about the data set itself. In order to do this, we will be using four machine learning methods : Random Forest, Boosted Trees, Logistic Regression, and Quadratic Discriminant Analysis (QDA).

We will first start with the Random Forest model. We will start by setting the mode and engine, create a grid, create our workflow, and tune our model. Afterwards, we will find the `best_complexity` to find our best fit model.


```{r}
class_forest_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

param_grid2 <- grid_regular(mtry(range = c(1, 8)), trees(range = c(1,8)), min_n(range = c(1,8)),  levels = 8)

forest_workflow <- workflow() %>%
  add_model(class_forest_spec %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>%
  add_recipe(bcancer_recipe)

tune_res_forest <- tune_grid(
  forest_workflow, 
  resamples = bcancer_fold, 
  grid = param_grid2, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_forest)

collect_metrics(tune_res_forest) %>% 
  arrange(desc(mean))


best_complexity <- select_best(tune_res_forest)
best_complexity



```

Note that approximately 85~90 percent is a good `roc_auc` value. In our random forest model, we can see that for each Minimal Node Size, the `roc_auc` values are all above 85 percent and the best `roc_auc` value is 0.9894460. Therefore, this is a very good model overall. 


#### Boosted Trees
For our second model, we will use boosted trees for our data. We will set the engine, set the mode, create a workflow, tune our model, and use `best_complexity3` to find the best fit model for our data. 

```{r}
boost_spec <- boost_tree(trees = 100, tree_depth = 4) %>%
  set_engine("xgboost") %>%
  set_mode("classification")


param_grid_boost <- grid_regular(trees(range = c(10, 50)),  levels = 10)

boost_workflow <- workflow() %>%
  add_model(boost_spec %>% set_args(trees = tune())) %>%
  add_recipe(bcancer_recipe)

tune_res_boost <- tune_grid(
  boost_workflow, 
  resamples = bcancer_fold, 
  grid = param_grid_boost, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_boost)

collect_metrics(tune_res_boost) %>% 
  arrange(desc(mean))

best_complexity3 <- select_best(tune_res_boost)
best_complexity3


```


We can see that the boosted trees model is also very good. Since most of the `roc_auc` values in our boosted trees is above 0.979, this means that this is also a very good model for our data set. Additionally, the best `roc_auc` value is 0.9826345 which is extremely good.

Now since our two models are very good, we will finalize both of our models by creating a final fit and a confusion matrix for further interpretation. We will first start with our random forest model.


```{r}

class_tree_final <- finalize_workflow(forest_workflow, best_complexity)

class_tree_final

class_tree_final_fit <- fit(class_tree_final, data = bcancer_train)


#confusion matrix heat map
augment(class_tree_final_fit, new_data = bcancer_test) %>%
  conf_mat(truth = diagnosis, estimate = .pred_class) %>%
  autoplot(type = "heatmap")


```


We will try to interpret the confusion matrix here. Recall that 0 represents benign and 1 represents malignant. Thus, 218 implies the number of incidents where benign patients are determined correctly as benign patients. On the other hand, 7 would represent the number of malignant patients considered to be benign patients and 9 would represent the number of benign patients as malignant patients. Lastly, 116 would represent the number of patients where they are correctly diagnosed as malignant patients as malignant patients. Recall that the case where malignant, which is positive, is diagnosed as benign which is negative, so 1 and 0 respectively, which is 7 in our case, is called a false negative, type 2 error. On the other hand, where benign patients are diagnosed as malignant patients are called a false positive, a type 1 error. 

Now, we will do the same thing for our boosted trees model.

```{r}
class_tree_final2 <- finalize_workflow(boost_workflow, best_complexity3)

class_tree_final2

class_tree_final_fit2 <- fit(class_tree_final2, data = bcancer_train)


#confusion matrix heat map
augment(class_tree_final_fit2, new_data = bcancer_train) %>%
  conf_mat(truth = diagnosis, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```


We can see that similar to our random forest model, our boosted trees model is also a very good model for our data set. We have less type 1 and type 2 errors. Our type 1 error here has 2 cases and our type 2 error here is has only 1 case. Also, 229 benign patients were correctly observed as benign patients, and 117 malignant patients were correctly observed as malignant patients.



#### Logistic Regression

Now, we will use a logistic regression for our third model. This is the following code for our logistic regression : 


```{r}
rec_poly <- recipe(diagnosis ~ ., data = bcancer_train) %>%
  step_poly( degree = 4)

lr_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

lr_poly_wf <- workflow() %>%
  add_model(lr_spec) %>%
  add_recipe(rec_poly)

lr_poly_fit <- fit(lr_poly_wf, data = bcancer_train)

predict(lr_poly_fit, new_data = bcancer_train)


```

Now, we will create a confusion matrix to see our type 1 and type 2 errors.



```{r}


augment(lr_poly_fit, new_data = bcancer_train) %>%
  conf_mat(truth = diagnosis, estimate = .pred_class)

augment(lr_poly_fit, new_data = bcancer_train) %>%
  conf_mat(truth = diagnosis, estimate = .pred_class) %>%
  autoplot(type = "heatmap")



```

Here, we can see that we have a very low type 1 and type 2 error. Therefore, we can assume here that this model is also pretty good.


In order to make sure that our model is a good fit, we will look at the accuracies and roc_auc values for this.

```{r}
log_acc <- predict(lr_poly_fit, new_data = bcancer_train, type = "class") %>% 
  bind_cols(bcancer_train %>% select(diagnosis)) %>% 
  accuracy(truth = diagnosis, estimate = .pred_class)
log_acc

log_test <- fit(lr_poly_wf, bcancer_test)
predict(log_test, new_data = bcancer_test, type = "class") %>% 
  bind_cols(bcancer_test %>% select(diagnosis)) %>% 
  accuracy(truth = diagnosis, estimate = .pred_class)

augment(log_test, new_data = bcancer_test) %>%
  roc_curve(diagnosis, .pred_0) %>%
  autoplot()

augment(log_test, new_data = bcancer_test) %>%
  roc_auc(diagnosis, .pred_0)
```
We can clearly see that this is also a very good model for our data set. By comparing the specificity and sensitivity, it is very obvious that our model is very well behaving. Our accuracy estimate is 0.9684814, which is very high. Additionally, our roc_auc value is 0.9938039 which is also extremely high. Therefore, the logistic regression model is a good fir for our data set. 



#### QDA

Now, we will look into our last model, qda model. We wil first set the mode, engine, workflow, and recipe. 

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(bcancer_recipe)

qda_fit <- fit(qda_wkflow, bcancer_train)

predict(qda_fit, new_data = bcancer_train, type = "prob")


```

```{r}
augment(qda_fit, new_data = bcancer_train) %>%
  conf_mat(truth = diagnosis, estimate = .pred_class) 

augment(qda_fit, new_data = bcancer_train) %>%
  conf_mat(truth = diagnosis, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

The confusion matrix here tells us that we have 3 observations of type 2 error and 15 observations of type 1 error. This is also a pretty low error for each case. 

In order to see how good our model is fitting, we will find the accuracies and roc_auc value for our qda model. 


```{r}

qda_acc <- augment(qda_fit, new_data = bcancer_train) %>%
  accuracy(truth = diagnosis, estimate = .pred_class)
qda_acc

predict(qda_fit, new_data = bcancer_test, type = "prob")

augment(qda_fit, new_data = bcancer_test) %>%
  conf_mat(truth = diagnosis, estimate = .pred_class) 

multi_metric <- metric_set(accuracy)

augment(qda_fit, new_data = bcancer_test) %>%
  multi_metric(truth = diagnosis, estimate = .pred_class)

augment(qda_fit, new_data = bcancer_test) %>%
  roc_curve(diagnosis, .pred_0) %>%
  autoplot()

augment(qda_fit, new_data = bcancer_test) %>%
  roc_auc(diagnosis, .pred_0)
```
The accuracy estimate for our qda model is 0.9484241 which is pretty high. By looking at our roc_curve, we can see that our model is a really good fit for our data set. Additionally, the roc_auc value estimate is 0.9912969 which is extremely high. Therefore, we conclude that the qda model is also a very good model for our data set. 


### Finding our best model

Now, we will find the best model for our data set. Clearly, all of our models performed pretty well, but we will find the best one to lower our error. Finding the best fit model is very significant for our project, because it is very important to diagnose the right cancer cell to our patient for correct treatment.

```{r}
collect_metrics(tune_res_boost) %>% 
  arrange(desc(mean))

collect_metrics(tune_res_forest) %>% 
  arrange(desc(mean))


augment(log_test, new_data = bcancer_test) %>%
  roc_auc(diagnosis, .pred_0)

augment(qda_fit, new_data = bcancer_test) %>%
  roc_auc(diagnosis, .pred_0)


roc_auc_values <- c(0.9826345, 0.9894460, 0.9938039, 0.9912969	)
models <- c("Random Forest", "Boosted Trees", "Logistic Regression","QDA")
results <- tibble(roc_auc_values = roc_auc_values, models = models)
results



```

The following table is the roc_auc values for all four models. We can see that the logistic regression model has the highest roc_auc values. Therefore, we conclude that the logistic regression model is the best fit for our model.


Now, we will try to fit the logistic regression model into our testing data set to see how it does. 


```{r}
predict(lr_poly_fit, new_data = bcancer_test, type = "prob")

augment(lr_poly_fit, new_data = bcancer_test) %>%
  conf_mat(truth = diagnosis, estimate = .pred_class) 

multi_metric <- metric_set(accuracy)

augment(lr_poly_fit, new_data = bcancer_test) %>%
  multi_metric(truth = diagnosis, estimate = .pred_class)

augment(lr_poly_fit, new_data = bcancer_test) %>%
  roc_curve(diagnosis, .pred_0) %>%
  autoplot()


```



We can see that we only have a total of 16 cases that were not predicted correctly. Additionally, the accuracy estimate is 0.9542857, which is pretty high. The roc_curve seems to look very good as well, showing a good performance.

Overall, we conclude that the logistic regression model is the best fit for our data set. 

## Conclusion


Our goal of this project was to create a model to correctly classify whether the breast cancer is a malignant or benign type with the help of our data set. For a breast cancer patient, the accuracy of determining the right diagnosis is very significant for their treatment. Therefore, finding the lowest error and highest accuracy, in other words, finding the best fit model is very critical in this field. 

Before we started fitting our models, we cleaned our data and split our data for the process. This helped reduce any type of error for our project. 

In this project, the following five models were used : random forest, boosted trees, logistic regression, and KNN. The five models were all very successful models with very low error. But since we are trying to minimize the error rate, we tried to find the best model for our project. We conclude that the boosted trees model is the best model for our project because it has the least number of type 1 and type 2 error. Since in biostatistics, reducing the type 1 and type 2 error is significant, the boosted tree models seems to be the best, having the least number of errors. 

Further research could be done by including other potentially significant predictors to our project, so that we could get more information than just malignant or benign cancer cells. In other words, we can go more in depth by including more information to gain more information about the patient about breast cancer. 

Overall, the research project with the breast cancer data set was very successful, determining which cancer cell, malignant or benign, a patient has. With high roc_auc values and accuracies, the models used would successfully determine the correct cancer cell with low error rate.
 




